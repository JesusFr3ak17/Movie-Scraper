{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7155a458",
   "metadata": {},
   "source": [
    "### For ToxicWap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5aec2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search for a movie: merlin\n",
      "Starting Loops\n",
      "['02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15']\n",
      "Looping through  02\n",
      "Looping through  03\n",
      "Looping through  04\n",
      "Looping through  05\n",
      "Looping through  06\n",
      "Looping through  07\n",
      "Looping through  08\n",
      "Looping through  09\n",
      "{'Merlin': 'https://ratedwap.com/new/1/Downloads/TV-Series/Merlin.html'}\n",
      "https://ratedwap.com/cms/sub_thumb/Merlin.jpg\n",
      "Link:  https://newtoxic.com/new/1/1567/Season-5/\n",
      "Text:  Season 5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m loads \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://newtoxic.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m loads_1 \u001b[38;5;241m=\u001b[39m BeautifulSoup(loads\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m load_results \u001b[38;5;241m=\u001b[39m \u001b[43mloads_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mul\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata-role\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlistview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(load_results)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import streamlit as st\n",
    "import re\n",
    "\n",
    "all_movie_list = {}\n",
    "search_term = str(input('Search for a movie: '))\n",
    "strip_search = str.lower(search_term[0])\n",
    "url = \"https://newtoxic.com/TV_Series/\" + strip_search + \".php\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "movie_present = False\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    text = link.get_text()\n",
    "    if href and search_term.lower() in href.lower():\n",
    "        all_movie_list[text] = href\n",
    "    elif search_term.lower() in text.lower():\n",
    "        all_movie_list[text] = href\n",
    "        movie_present = href\n",
    "#print(all_movie_list)\n",
    "\n",
    "if movie_present == False:\n",
    "    print('Starting Loops')\n",
    "    movie_list = soup.find('ul', {'data-role': 'listview'}).find_all('a')\n",
    "    last_index = int(movie_list[-1].text)\n",
    "    other_pages = [\"%.2d\" % i for i in range(2, 16)]\n",
    "    print(other_pages)\n",
    "    for i in other_pages:\n",
    "        print('Looping through ', i)\n",
    "        url = \"https://newtoxic.com/TV_Series/\" + strip_search + i + \".php\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text()\n",
    "            if href and search_term.lower() in href.lower():\n",
    "                all_movie_list[text] = href\n",
    "                movie_present = True\n",
    "                break\n",
    "            elif search_term.lower() in text.lower():\n",
    "                all_movie_list[text] = href\n",
    "                movie_present = True\n",
    "                break\n",
    "        if movie_present == True:\n",
    "            movie_title = text\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "print(all_movie_list)\n",
    "\n",
    "for key in all_movie_list:\n",
    "    url = all_movie_list[key]\n",
    "    response = requests.get(url)\n",
    "    search_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = search_response.find('ul', {'data-role': 'listview'}).find_all('a')\n",
    "    print(\"https://ratedwap.com/cms/sub_thumb/\" + str(movie_title) + \".jpg\")\n",
    "    for link in links:\n",
    "        print(\"Link: \", 'https://newtoxic.com' + link['href'])\n",
    "        print(\"Text: \", link.text)\n",
    "        loads = requests.get('https://newtoxic.com' + link['href'])\n",
    "        loads_1 = BeautifulSoup(loads.text, 'html.parser')\n",
    "        load_results = loads_1.find('ul', {'data-role': 'listview'}).find_all('a')\n",
    "        print(load_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8c52525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import streamlit as st\n",
    "import re\n",
    "\n",
    "# Function to extract all movies from the search result\n",
    "def extract_movies(search_term):\n",
    "    all_movie_list = {}\n",
    "    strip_search = str.lower(search_term[0])\n",
    "    url = \"https://newtoxic.com/TV_Series/\" + strip_search + \".php\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    movie_present = False\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "        text = link.get_text()\n",
    "        if href and search_term.lower() in href.lower():\n",
    "            all_movie_list[text] = href\n",
    "        elif search_term.lower() in text.lower():\n",
    "            all_movie_list[text] = href\n",
    "            movie_present = href\n",
    "    if movie_present == False:\n",
    "        last_index = int(soup.find('ul', {'data-role': 'listview'}).find_all('a')[-1].text)\n",
    "        other_pages = [\"%.2d\" % i for i in range(2, 16)]\n",
    "        for i in other_pages:\n",
    "            url = \"https://newtoxic.com/TV_Series/\" + strip_search + i + \".php\"\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a'):\n",
    "                href = link.get('href')\n",
    "                text = link.get_text()\n",
    "                if href and search_term.lower() in href.lower():\n",
    "                    all_movie_list[text] = href\n",
    "                    movie_present = True\n",
    "                    break\n",
    "                elif search_term.lower() in text.lower():\n",
    "                    all_movie_list[text] = href\n",
    "                    movie_present = True\n",
    "                    break\n",
    "            if movie_present == True:\n",
    "                break\n",
    "    return all_movie_list\n",
    "\n",
    "# Function to get seasons for a movie\n",
    "def get_seasons(url):\n",
    "    response = requests.get(url)\n",
    "    search_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    seasons = []\n",
    "    for link in search_response.find('ul', {'data-role': 'listview'}).find_all('a'):\n",
    "        href = link.get('href')\n",
    "        text = link.get_text()\n",
    "        if 'Season' in text:\n",
    "            seasons.append((text, href))\n",
    "    return seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09c0cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://newtoxic.com/new/1/Videos/TV-Series/Hello-Ladies.html\n"
     ]
    }
   ],
   "source": [
    "dets = extract_movies(\"hello\")\n",
    "for _, url in dets.items():\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "031aa625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "def download_season(search_term):\n",
    "    dets = extract_movies(search_term)\n",
    "    for _, url in dets.items():\n",
    "        season_url = url\n",
    "    parsed_url = urlparse(season_url)\n",
    "    site_name = parsed_url.hostname\n",
    "    print(site_name)\n",
    "    response = requests.get(url)\n",
    "    search_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    #print(search_response)\n",
    "    seasons = get_seasons(season_url)\n",
    "    #print(seasons)\n",
    "    urls = seasons[0][1]\n",
    "    new = 'https://' + str(site_name) + urls\n",
    "    print(new)\n",
    "    response = requests.get(new)\n",
    "    search_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    #print(search_response)\n",
    "    all_episodes = []\n",
    "    #print(search_response)\n",
    "    for link in search_response.find('ul', {'data-role': 'listview'}).find_all('a'):\n",
    "        href = link.get('href')\n",
    "        text = link.get_text()\n",
    "        season_download = 'https://' + str(site_name) + href\n",
    "        all_episodes.append((text, season_download))\n",
    "    a_elements = search_response.find_all('a')\n",
    "    numbers = [int(a.get_text()) for a in a_elements if a.get_text().isdigit()]\n",
    "    # Find the highest number\n",
    "    highest_number = max(numbers)\n",
    "    all_elements = []\n",
    "    for link in search_response.find_all('a'):\n",
    "        if link.text.isdigit():\n",
    "            all_elements.append(link.get('href'))\n",
    "    print(all_elements)\n",
    "    for i in range(1, highest_number):\n",
    "        print(i)\n",
    "        url = 'https://' + str(site_name) + all_elements[i]\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find('ul', {'data-role': 'listview'}).find_all('a'):\n",
    "            href = link.get('href')\n",
    "            text = link.get_text()\n",
    "            season_download = 'https://' + str(site_name) + href\n",
    "            all_episodes.append((text, season_download))\n",
    "    response = requests.get(all_episodes[0][1])\n",
    "    search_response = BeautifulSoup(response.text, 'html.parser')\n",
    "    a_tag = search_response.find('a', text='Download')\n",
    "    href = a_tag.get('href')\n",
    "    download_link = 'https://' + str(site_name) + href\n",
    "    for i in all_episodes:\n",
    "        print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58fe92c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratedwap.com\n",
      "https://ratedwap.com/new/1/1567/Season-5/\n",
      "['/new/1/1567/Season-5/', '/new/2/1567/Season-5/']\n",
      "1\n",
      "https://ratedwap.com/new/2/1567/Season-5/\n",
      "\n",
      "Merlin S05E05 Toxicwap.com .mp4\n",
      "100.62 MB\n",
      "\n",
      "\n",
      "Merlin S05E13 ToxicWap.com .mp4\n",
      "107.87 MB\n",
      "\n",
      "\n",
      "Merlin S05E11 ToxicWap.com .mp4\n",
      "97.66 MB\n",
      "\n",
      "\n",
      "Merlin S05E10 ToxicWap.com .mp4\n",
      "97.03 MB\n",
      "\n",
      "\n",
      "Merlin S05E04 ToxicWap.com .mp4\n",
      "97.29 MB\n",
      "\n",
      "\n",
      "Merlin S05E07 ToxicWap.com .mp4\n",
      "97.7 MB\n",
      "\n",
      "\n",
      "Merlin S05E06 ToxicWap.com .mp4\n",
      "95.55 MB\n",
      "\n",
      "\n",
      "Merlin S05E02 ToxicWap.com .mp4\n",
      "98.35 MB\n",
      "\n",
      "\n",
      "Merlin S05E01 ToxicWap.com .mp4\n",
      "75.76 MB\n",
      "\n",
      "\n",
      "Merlin S05E12 ToxicWap.com .mp4\n",
      "98.09 MB\n",
      "\n",
      "\n",
      "Merlin S05E09 ToxicWap.com .mp4\n",
      "98.59 MB\n",
      "\n",
      "\n",
      "Merlin S05E08 ToxicWap.com .mp4\n",
      "97.06 MB\n",
      "\n",
      "\n",
      "Merlin S05E03 ToxicWap.com .mp4\n",
      "72.99 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "download_season('merlin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c4cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
